{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom Training Loops in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to implement a basic custom training loop in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, pillow, optree, opt_einsum, numpy, mdurl, markdown, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, ml_dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 h5py-3.14.0 keras-3.11.3 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 numpy-2.3.3 opt_einsum-3.4.0 optree-0.17.0 pillow-11.3.0 protobuf-6.32.1 rich-14.1.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:14:55.654260: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-03 12:14:55.654730: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-03 12:14:55.735089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-03 12:14:57.423270: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-03 12:14:57.424116: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-03 12:14:58.686246: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the model: \n",
    "\n",
    "Create a simple neural network model with a Flatten layer followed by two Dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3503353595733643\n",
      "Epoch 1 Step 200: Loss = 0.4006781578063965\n",
      "Epoch 1 Step 400: Loss = 0.17092105746269226\n",
      "Epoch 1 Step 600: Loss = 0.1759812980890274\n",
      "Epoch 1 Step 800: Loss = 0.17334437370300293\n",
      "Epoch 1 Step 1000: Loss = 0.39219963550567627\n",
      "Epoch 1 Step 1200: Loss = 0.14793170988559723\n",
      "Epoch 1 Step 1400: Loss = 0.22093743085861206\n",
      "Epoch 1 Step 1600: Loss = 0.21817736327648163\n",
      "Epoch 1 Step 1800: Loss = 0.14732393622398376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:15:59.480962: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.06290686130523682\n",
      "Epoch 2 Step 200: Loss = 0.182691290974617\n",
      "Epoch 2 Step 400: Loss = 0.10947165638208389\n",
      "Epoch 2 Step 600: Loss = 0.04077688232064247\n",
      "Epoch 2 Step 800: Loss = 0.12160421162843704\n",
      "Epoch 2 Step 1000: Loss = 0.2852374315261841\n",
      "Epoch 2 Step 1200: Loss = 0.06440895795822144\n",
      "Epoch 2 Step 1400: Loss = 0.13296350836753845\n",
      "Epoch 2 Step 1600: Loss = 0.16127042472362518\n",
      "Epoch 2 Step 1800: Loss = 0.08444298803806305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:16:53.961192: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with accuracy: \n",
    "\n",
    "Track the accuracy during training and print it at regular intervals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3311169147491455 Accuracy = 0.21875\n",
      "Epoch 1 Step 200: Loss = 0.4061763286590576 Accuracy = 0.8339552283287048\n",
      "Epoch 1 Step 400: Loss = 0.1986168473958969 Accuracy = 0.8676745891571045\n",
      "Epoch 1 Step 600: Loss = 0.16148197650909424 Accuracy = 0.8825395107269287\n",
      "Epoch 1 Step 800: Loss = 0.14908674359321594 Accuracy = 0.8945848941802979\n",
      "Epoch 1 Step 1000: Loss = 0.4219292998313904 Accuracy = 0.9019418358802795\n",
      "Epoch 1 Step 1200: Loss = 0.17234566807746887 Accuracy = 0.9083576202392578\n",
      "Epoch 1 Step 1400: Loss = 0.19064456224441528 Accuracy = 0.9131423830986023\n",
      "Epoch 1 Step 1600: Loss = 0.22506362199783325 Accuracy = 0.9161266684532166\n",
      "Epoch 1 Step 1800: Loss = 0.14698009192943573 Accuracy = 0.919870913028717\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.06380803883075714 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.15341858565807343 Accuracy = 0.9603545069694519\n",
      "Epoch 2 Step 400: Loss = 0.10355117917060852 Accuracy = 0.9558135867118835\n",
      "Epoch 2 Step 600: Loss = 0.06431642919778824 Accuracy = 0.9586626291275024\n",
      "Epoch 2 Step 800: Loss = 0.10220001637935638 Accuracy = 0.9603230357170105\n",
      "Epoch 2 Step 1000: Loss = 0.25613683462142944 Accuracy = 0.96107017993927\n",
      "Epoch 2 Step 1200: Loss = 0.06259706616401672 Accuracy = 0.9620108008384705\n",
      "Epoch 2 Step 1400: Loss = 0.0993565022945404 Accuracy = 0.9627944231033325\n",
      "Epoch 2 Step 1600: Loss = 0.1685713678598404 Accuracy = 0.9624258279800415\n",
      "Epoch 2 Step 1800: Loss = 0.08692743629217148 Accuracy = 0.9632322192192078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:19:02.348049: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.04305000975728035 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.10857294499874115 Accuracy = 0.9751243591308594\n",
      "Epoch 3 Step 400: Loss = 0.09665383398532867 Accuracy = 0.9716334342956543\n",
      "Epoch 3 Step 600: Loss = 0.05729333311319351 Accuracy = 0.973013699054718\n",
      "Epoch 3 Step 800: Loss = 0.07083623856306076 Accuracy = 0.9733145833015442\n",
      "Epoch 3 Step 1000: Loss = 0.1848476082086563 Accuracy = 0.9736513495445251\n",
      "Epoch 3 Step 1200: Loss = 0.03761734068393707 Accuracy = 0.9745004177093506\n",
      "Epoch 3 Step 1400: Loss = 0.051253654062747955 Accuracy = 0.9749955534934998\n",
      "Epoch 3 Step 1600: Loss = 0.12284112721681595 Accuracy = 0.9746057391166687\n",
      "Epoch 3 Step 1800: Loss = 0.04738309234380722 Accuracy = 0.9751179814338684\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.02564319781959057 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.09650999307632446 Accuracy = 0.9830535054206848\n",
      "Epoch 4 Step 400: Loss = 0.07234936207532883 Accuracy = 0.9808291792869568\n",
      "Epoch 4 Step 600: Loss = 0.05362343043088913 Accuracy = 0.981697142124176\n",
      "Epoch 4 Step 800: Loss = 0.0455465167760849 Accuracy = 0.9817025661468506\n",
      "Epoch 4 Step 1000: Loss = 0.1487584412097931 Accuracy = 0.9820803999900818\n",
      "Epoch 4 Step 1200: Loss = 0.02211592346429825 Accuracy = 0.9824365377426147\n",
      "Epoch 4 Step 1400: Loss = 0.022825371474027634 Accuracy = 0.9824232459068298\n",
      "Epoch 4 Step 1600: Loss = 0.062424805015325546 Accuracy = 0.9819644093513489\n",
      "Epoch 4 Step 1800: Loss = 0.027024805545806885 Accuracy = 0.982422947883606\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.02859162725508213 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.06919783353805542 Accuracy = 0.9860074520111084\n",
      "Epoch 5 Step 400: Loss = 0.04613381624221802 Accuracy = 0.9851153492927551\n",
      "Epoch 5 Step 600: Loss = 0.05591543763875961 Accuracy = 0.9864288568496704\n",
      "Epoch 5 Step 800: Loss = 0.028138259425759315 Accuracy = 0.9863061904907227\n",
      "Epoch 5 Step 1000: Loss = 0.13087661564350128 Accuracy = 0.9869817495346069\n",
      "Epoch 5 Step 1200: Loss = 0.01946294493973255 Accuracy = 0.9872762560844421\n",
      "Epoch 5 Step 1400: Loss = 0.012454700656235218 Accuracy = 0.987285852432251\n",
      "Epoch 5 Step 1600: Loss = 0.043139245361089706 Accuracy = 0.9868441820144653\n",
      "Epoch 5 Step 1800: Loss = 0.02173200622200966 Accuracy = 0.987003743648529\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function, Optimizer, and Metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with custom callback: \n",
    "\n",
    "Create a custom callback to log additional metrics at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.5743565559387207 Accuracy = 0.03125\n",
      "Epoch 1 Step 200: Loss = 0.33065295219421387 Accuracy = 0.8310012221336365\n",
      "Epoch 1 Step 400: Loss = 0.20401683449745178 Accuracy = 0.8659601211547852\n",
      "Epoch 1 Step 600: Loss = 0.18401549756526947 Accuracy = 0.8818115592002869\n",
      "Epoch 1 Step 800: Loss = 0.16146203875541687 Accuracy = 0.8941166996955872\n",
      "Epoch 1 Step 1000: Loss = 0.4446086883544922 Accuracy = 0.9014111161231995\n",
      "Epoch 1 Step 1200: Loss = 0.19648730754852295 Accuracy = 0.9080713987350464\n",
      "Epoch 1 Step 1400: Loss = 0.2701543867588043 Accuracy = 0.9132093191146851\n",
      "Epoch 1 Step 1600: Loss = 0.2075517624616623 Accuracy = 0.9166145920753479\n",
      "Epoch 1 Step 1800: Loss = 0.22302457690238953 Accuracy = 0.9206517338752747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:23:49.069027: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.05208666995167732, accuracy: 0.9225666522979736\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08774880319833755 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.14361387491226196 Accuracy = 0.9625310897827148\n",
      "Epoch 2 Step 400: Loss = 0.09703484177589417 Accuracy = 0.9601776599884033\n",
      "Epoch 2 Step 600: Loss = 0.05484282225370407 Accuracy = 0.961730420589447\n",
      "Epoch 2 Step 800: Loss = 0.09409403800964355 Accuracy = 0.9626248478889465\n",
      "Epoch 2 Step 1000: Loss = 0.28888529539108276 Accuracy = 0.9623813629150391\n",
      "Epoch 2 Step 1200: Loss = 0.10505108535289764 Accuracy = 0.9630516171455383\n",
      "Epoch 2 Step 1400: Loss = 0.16453483700752258 Accuracy = 0.9639096856117249\n",
      "Epoch 2 Step 1600: Loss = 0.17460127174854279 Accuracy = 0.9636750221252441\n",
      "Epoch 2 Step 1800: Loss = 0.13511590659618378 Accuracy = 0.9644121527671814\n",
      "End of epoch 2, loss: 0.05944150686264038, accuracy: 0.9650833606719971\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Add Hidden Layers \n",
    "\n",
    "Next, you will add a couple of hidden layers to your model. Hidden layers help the model learn complex patterns in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Define the output layer \n",
    "\n",
    "Finally, you will define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Create the Model \n",
    "\n",
    "Now, you will create the model by specifying the input and output layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Compile the Model \n",
    "\n",
    "Before training the model, you need to compile it. You will specify the loss function, optimizer, and evaluation metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Train the Model \n",
    "\n",
    "You can now train the model on some training data. For this example, let's assume `X_train` is our training input data and `y_train` is the corresponding labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Evaluate the Model \n",
    "\n",
    "After training, you can evaluate the model on test data to see how well it performs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises \n",
    "\n",
    "### Exercise 1: Basic Custom Training Loop \n",
    "\n",
    "#### Objective: Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric \n",
    "\n",
    "#### Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging \n",
    "\n",
    "#### Objective: Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "3. Implement the custom training loop with the custom callback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "class CustomCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()  # Updated method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "\n",
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Explanation of Hyperparameter Tuning\n",
    "\n",
    "**Addition to Explanation:** Add a note explaining the purpose of num_trials in the hyperparameter tuning context:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
